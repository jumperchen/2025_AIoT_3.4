{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SageMaker Training Job 訓練 Ultralytics YOLO11（YOLOv11）\n",
        "\n",
        "此 Notebook 會：\n",
        "1. 生成訓練程式 `train.py` 與 `requirements.txt`\n",
        "2. 以 **SageMaker PyTorch Estimator** 啟動 Training Job\n",
        "3. 在 Notebook 內**串流顯示訓練日誌**，並**嘗試動態解析每個 epoch 指標，畫出收斂曲線**\n",
        "4. 訓練完成後下載 artifacts（含 `best.pt`、`runs/`、`results.csv`、`results.png`）並顯示完整曲線\n",
        "\n",
        "> 注意：動態曲線解析依賴 Ultralytics 日誌格式；若版本輸出不同，可能解析不到，但**訓練完成後**一定能用 `results.csv/results.png` 畫出完整收斂曲線。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) 先決條件\n",
        "\n",
        "- 已在 SageMaker Studio / Notebook Instance 執行\n",
        "- IAM role 具備 SageMaker 與 S3 權限\n",
        "- 你已把 YOLO 格式資料集打包成 zip 放在 S3\n",
        "\n",
        "資料 zip 內部建議結構：\n",
        "```\n",
        "mydata/\n",
        "  images/train\n",
        "  images/val\n",
        "  labels/train\n",
        "  labels/val\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, tarfile, re, time\n",
        "from pathlib import Path\n",
        "\n",
        "import boto3\n",
        "import sagemaker\n",
        "from sagemaker.pytorch import PyTorch\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.s3 import S3Downloader\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output, Image\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "region = sess.boto_region_name\n",
        "role = sagemaker.get_execution_role()\n",
        "sm = boto3.client('sagemaker', region_name=region)\n",
        "logs = boto3.client('logs', region_name=region)\n",
        "\n",
        "print('region:', region)\n",
        "print('default bucket:', sess.default_bucket())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 參數設定（請改成你的 S3 路徑與類別名稱）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== 你要改的地方 ======\n",
        "S3_DATA_ZIP = 's3://YOUR-BUCKET/datasets/mydata.zip'  # <<<<<< 改成你的資料集\n",
        "DATA_ZIP_FILENAME = 'mydata.zip'                     # zip 檔名\n",
        "\n",
        "# 類別名稱（請改成你的 classes）\n",
        "CLASS_NAMES = ['class0', 'class1']\n",
        "\n",
        "# YOLO11 權重（可改 yolo11s.pt / yolo11m.pt / yolo11l.pt / yolo11x.pt）\n",
        "YOLO_MODEL = 'yolo11n.pt'\n",
        "\n",
        "# 訓練超參數\n",
        "EPOCHS = 50\n",
        "IMGSZ = 640\n",
        "BATCH = 16\n",
        "WORKERS = 4\n",
        "\n",
        "# 訓練硬體（推薦 GPU：g5 / g4dn）\n",
        "INSTANCE_TYPE = 'ml.g5.2xlarge'\n",
        "INSTANCE_COUNT = 1\n",
        "\n",
        "# 輸出到 S3\n",
        "OUTPUT_S3 = f\"s3://{sess.default_bucket()}/yolo11/output/\"\n",
        "\n",
        "print('S3_DATA_ZIP:', S3_DATA_ZIP)\n",
        "print('OUTPUT_S3:', OUTPUT_S3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 生成訓練程式碼（train.py）與 requirements.txt\n",
        "\n",
        "重點：\n",
        "- SageMaker 會把 `training` channel 掛載在 `/opt/ml/input/data/training/`\n",
        "- 你要把 artifacts 寫到 `/opt/ml/model/`，SageMaker 才會上傳到 `output_path`\n",
        "- 我們把 Ultralytics 的 `runs/` 放到 `/opt/ml/model/runs/`，方便事後下載 `results.csv` / `results.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_dir = Path('src')\n",
        "src_dir.mkdir(exist_ok=True)\n",
        "\n",
        "train_py = r'''\n",
        "import argparse\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data_zip', type=str, default='mydata.zip')\n",
        "    p.add_argument('--model', type=str, default='yolo11n.pt')\n",
        "    p.add_argument('--epochs', type=int, default=50)\n",
        "    p.add_argument('--imgsz', type=int, default=640)\n",
        "    p.add_argument('--batch', type=int, default=16)\n",
        "    p.add_argument('--workers', type=int, default=4)\n",
        "    p.add_argument('--class_names_json', type=str, required=True)\n",
        "    args = p.parse_args()\n",
        "\n",
        "    sm_data_dir = Path('/opt/ml/input/data/training')\n",
        "    sm_model_dir = Path('/opt/ml/model')\n",
        "    sm_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    work_dir = Path('/tmp/data')\n",
        "    work_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    zip_path = sm_data_dir / args.data_zip\n",
        "    if not zip_path.exists():\n",
        "        raise FileNotFoundError(f'Dataset zip not found: {zip_path}')\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(work_dir)\n",
        "\n",
        "    entries = [d for d in work_dir.iterdir() if d.is_dir()]\n",
        "    dataset_root = entries[0] if entries else work_dir\n",
        "\n",
        "    class_names = __import__('json').loads(args.class_names_json)\n",
        "    data_yaml = {\n",
        "        'path': str(dataset_root),\n",
        "        'train': str(dataset_root / 'images/train'),\n",
        "        'val': str(dataset_root / 'images/val'),\n",
        "        'names': class_names,\n",
        "    }\n",
        "    yaml_path = work_dir / 'data.yaml'\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.safe_dump(data_yaml, f)\n",
        "\n",
        "    model = YOLO(args.model)\n",
        "    project_dir = sm_model_dir / 'runs'\n",
        "    model.train(\n",
        "        data=str(yaml_path),\n",
        "        epochs=args.epochs,\n",
        "        imgsz=args.imgsz,\n",
        "        batch=args.batch,\n",
        "        workers=args.workers,\n",
        "        project=str(project_dir),\n",
        "        name='train',\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    best_pt = project_dir / 'train' / 'weights' / 'best.pt'\n",
        "    if best_pt.exists():\n",
        "        shutil.copy2(best_pt, sm_model_dir / 'best.pt')\n",
        "    last_pt = project_dir / 'train' / 'weights' / 'last.pt'\n",
        "    if last_pt.exists():\n",
        "        shutil.copy2(last_pt, sm_model_dir / 'last.pt')\n",
        "\n",
        "    print('Training complete. Saved best.pt to', sm_model_dir / 'best.pt')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''\n",
        "\n",
        "(src_dir / 'train.py').write_text(train_py, encoding='utf-8')\n",
        "(src_dir / 'requirements.txt').write_text('ultralytics\\nopencv-python\\npyyaml\\n', encoding='utf-8')\n",
        "print('Wrote:', src_dir / 'train.py')\n",
        "print('Wrote:', src_dir / 'requirements.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 建立 Estimator 並啟動 Training Job（非阻塞）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    'data_zip': DATA_ZIP_FILENAME,\n",
        "    'model': YOLO_MODEL,\n",
        "    'epochs': EPOCHS,\n",
        "    'imgsz': IMGSZ,\n",
        "    'batch': BATCH,\n",
        "    'workers': WORKERS,\n",
        "    'class_names_json': json.dumps(CLASS_NAMES, ensure_ascii=False)\n",
        "}\n",
        "\n",
        "est = PyTorch(\n",
        "    entry_point='train.py',\n",
        "    source_dir=str(src_dir),\n",
        "    role=role,\n",
        "    framework_version='2.2',\n",
        "    py_version='py310',\n",
        "    instance_count=INSTANCE_COUNT,\n",
        "    instance_type=INSTANCE_TYPE,\n",
        "    hyperparameters=hyperparameters,\n",
        "    output_path=OUTPUT_S3,\n",
        ")\n",
        "\n",
        "job_name = f\"yolo11-train-{int(time.time())}\"\n",
        "print('TrainingJobName:', job_name)\n",
        "\n",
        "est.fit(\n",
        "    inputs={'training': TrainingInput(S3_DATA_ZIP, content_type='application/zip')},\n",
        "    job_name=job_name,\n",
        "    wait=False,\n",
        ")\n",
        "print('已送出 Training Job。接著跑下一格開始動態監控。')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 動態顯示訓練過程與收斂曲線（即時）\n",
        "\n",
        "會從 CloudWatch Logs 拉取最新日誌並嘗試解析 epoch 指標。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_training_job_status(job_name: str):\n",
        "    desc = sm.describe_training_job(TrainingJobName=job_name)\n",
        "    return desc['TrainingJobStatus'], desc\n",
        "\n",
        "def find_log_stream(job_name: str, log_group='/aws/sagemaker/TrainingJobs'):\n",
        "    paginator = logs.get_paginator('describe_log_streams')\n",
        "    for page in paginator.paginate(logGroupName=log_group, logStreamNamePrefix=job_name):\n",
        "        streams = page.get('logStreams', [])\n",
        "        if streams:\n",
        "            streams = sorted(streams, key=lambda s: s.get('lastEventTimestamp', 0), reverse=True)\n",
        "            return streams[0]['logStreamName']\n",
        "    return None\n",
        "\n",
        "def fetch_logs(log_group, log_stream, next_token=None):\n",
        "    kwargs = dict(logGroupName=log_group, logStreamName=log_stream, startFromHead=True)\n",
        "    if next_token:\n",
        "        kwargs['nextToken'] = next_token\n",
        "    resp = logs.get_log_events(**kwargs)\n",
        "    return resp.get('events', []), resp.get('nextForwardToken')\n",
        "\n",
        "# best-effort 解析：epoch + 後面 5 個浮點數（版本不同可能不完全一致）\n",
        "epoch_metrics = []\n",
        "epoch_line_re = re.compile(r\"^\\s*(\\d+)\\s+([0-9.eE+-]+)\\s+([0-9.eE+-]+)\\s+([0-9.eE+-]+)\\s+([0-9.eE+-]+)\\s+([0-9.eE+-]+)\")\n",
        "\n",
        "def try_parse_epoch_line(msg: str):\n",
        "    m = epoch_line_re.match(msg)\n",
        "    if not m:\n",
        "        return None\n",
        "    epoch = int(m.group(1))\n",
        "    vals = [float(m.group(i)) for i in range(2, 7)]\n",
        "    return {'epoch': epoch, 'v1': vals[0], 'v2': vals[1], 'v3': vals[2], 'v4': vals[3], 'v5': vals[4]}\n",
        "\n",
        "log_group = '/aws/sagemaker/TrainingJobs'\n",
        "log_stream = None\n",
        "token = None\n",
        "seen = set()\n",
        "tail = []\n",
        "\n",
        "print('尋找 CloudWatch Log Stream...')\n",
        "for _ in range(60):\n",
        "    log_stream = find_log_stream(job_name, log_group=log_group)\n",
        "    if log_stream:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "if not log_stream:\n",
        "    raise RuntimeError('找不到 log stream。請稍後再跑一次這格。')\n",
        "\n",
        "print('log_stream:', log_stream)\n",
        "\n",
        "while True:\n",
        "    status, desc = get_training_job_status(job_name)\n",
        "    events, token = fetch_logs(log_group, log_stream, token)\n",
        "    new_lines = []\n",
        "    for e in events:\n",
        "        key = (e.get('timestamp'), e.get('message'))\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        msg = (e.get('message') or '').rstrip('\\n')\n",
        "        if msg:\n",
        "            new_lines.append(msg)\n",
        "            parsed = try_parse_epoch_line(msg)\n",
        "            if parsed:\n",
        "                if not epoch_metrics or parsed['epoch'] != epoch_metrics[-1]['epoch']:\n",
        "                    epoch_metrics.append(parsed)\n",
        "\n",
        "    tail = (tail + new_lines)[-50:]\n",
        "    clear_output(wait=True)\n",
        "    print('TrainingJob:', job_name)\n",
        "    print('Status:', status)\n",
        "    print('--- logs tail ---')\n",
        "    for line in tail:\n",
        "        print(line)\n",
        "\n",
        "    if len(epoch_metrics) >= 2:\n",
        "        df_live = pd.DataFrame(epoch_metrics).sort_values('epoch')\n",
        "        display(df_live.tail(10))\n",
        "        plt.figure()\n",
        "        plt.plot(df_live['epoch'], df_live['v1'], label='v1')\n",
        "        plt.plot(df_live['epoch'], df_live['v2'], label='v2')\n",
        "        plt.plot(df_live['epoch'], df_live['v3'], label='v3')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend()\n",
        "        plt.title('Live convergence (best-effort parsed from logs)')\n",
        "        plt.show()\n",
        "\n",
        "    if status in ['Completed', 'Failed', 'Stopped']:\n",
        "        print('\\nTraining job finished with status:', status)\n",
        "        if status != 'Completed':\n",
        "            print('FailureReason:', desc.get('FailureReason'))\n",
        "        break\n",
        "\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) 下載 artifacts 並畫出完整收斂曲線（results.csv / results.png）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    status, _ = get_training_job_status(job_name)\n",
        "    if status in ['Completed', 'Failed', 'Stopped']:\n",
        "        print('Final status:', status)\n",
        "        break\n",
        "    time.sleep(20)\n",
        "\n",
        "if status != 'Completed':\n",
        "    raise RuntimeError(f'Training job not completed: {status}')\n",
        "\n",
        "desc = sm.describe_training_job(TrainingJobName=job_name)\n",
        "model_artifact = desc['ModelArtifacts']['S3ModelArtifacts']\n",
        "print('Model artifact:', model_artifact)\n",
        "\n",
        "out_dir = Path('artifacts') / job_name\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "S3Downloader.download(model_artifact, str(out_dir))\n",
        "local_tar = out_dir / 'model.tar.gz'\n",
        "print('Downloaded:', local_tar)\n",
        "\n",
        "with tarfile.open(local_tar, 'r:gz') as t:\n",
        "    t.extractall(path=out_dir)\n",
        "\n",
        "best_pt = out_dir / 'best.pt'\n",
        "results_png = out_dir / 'runs' / 'train' / 'results.png'\n",
        "results_csv = out_dir / 'runs' / 'train' / 'results.csv'\n",
        "\n",
        "print('best.pt exists:', best_pt.exists())\n",
        "print('results.png exists:', results_png.exists())\n",
        "print('results.csv exists:', results_csv.exists())\n",
        "\n",
        "if results_png.exists():\n",
        "    display(Image(filename=str(results_png)))\n",
        "\n",
        "if results_csv.exists():\n",
        "    df = pd.read_csv(results_csv)\n",
        "    display(df.head())\n",
        "\n",
        "    cols = [c for c in df.columns if c.lower() not in ['epoch', 'time']]\n",
        "    x = df['epoch'] if 'epoch' in df.columns else range(len(df))\n",
        "\n",
        "    to_plot = cols[:8]\n",
        "    plt.figure()\n",
        "    for c in to_plot:\n",
        "        plt.plot(x, df[c], label=c)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend()\n",
        "    plt.title('Convergence curves (from results.csv)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('找不到 results.csv；請確認 runs/ 是否有被寫入 /opt/ml/model/runs')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}